{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5cbd834-8404-4ad6-bc42-3651885f52ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-21T16:24:43.467980Z",
     "iopub.status.busy": "2025-11-21T16:24:43.467703Z",
     "iopub.status.idle": "2025-11-21T16:25:41.778682Z",
     "shell.execute_reply": "2025-11-21T16:25:41.778249Z",
     "shell.execute_reply.started": "2025-11-21T16:24:43.467951Z"
    },
    "executionRoleArn": "arn:aws:iam::495736631375:role/trading_emr_serverless_service_role",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e06a86ae2bc440758444524c624e0ffc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr><th>ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>0</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"https://spark-live-ui.emr-serverless.amazonaws.com\" class=\"emr-proxy-link\" emr-runtime=\"emr-serverless\" emr-resource=\"00g1a44i53l0e90u\" application-id=\"00g19h06fkjbvt0t\">Link</a></td><td></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# file: features_eth_1s.py  (run on Glue/EMR or local Spark with S3 access)\n",
    "from pyspark.sql import SparkSession, functions as F, Window as W\n",
    "\n",
    "spark = (SparkSession.builder.appName(\"eth_features_1s\")\n",
    "         .config(\"spark.sql.session.timeZone\", \"UTC\")\n",
    "         .getOrCreate())\n",
    "\n",
    "start_dt, end_dt = \"2024-02-16\", \"2024-02-23\"   # <- adjust / parametrize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73627606-9d63-45ac-838f-66a5e634a9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "    CREATE EXTERNAL TABLE IF NOT EXISTS crypto_data.ethereum_trades (\n",
    "        price DOUBLE,  -- ← Force DOUBLE here\n",
    "        -- other columns\n",
    "    )\n",
    "    PARTITIONED BY (dt STRING)\n",
    "    STORED AS PARQUET\n",
    "    LOCATION 's3://ethereum-kbarczak/cryptolake/raw/trades/'\n",
    "    TBLPROPERTIES (\n",
    "        'parquet.enable.dictionary'='true',\n",
    "        'parquet.compression'='SNAPPY'\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"MSCK REPAIR TABLE crypto_data.ethereum_trades\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5546cde-b12a-416c-909a-0693ba23935a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T20:59:36.774546Z",
     "iopub.status.busy": "2025-11-19T20:59:36.774318Z",
     "iopub.status.idle": "2025-11-19T20:59:36.824439Z",
     "shell.execute_reply": "2025-11-19T20:59:36.823846Z",
     "shell.execute_reply.started": "2025-11-19T20:59:36.774520Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9e4d0068fa44dfb8fa4b260307f7b3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def resample_timeseries(df, cols, interval_us, timestamp_col=\"ts\", group_cols=None, \n",
    "                       return_as_timestamp=False, input_unit_us=1):\n",
    "    \"\"\"\n",
    "    Resamples a time-series DataFrame to a specified interval using last-observation-carried-forward (LOCF).\n",
    "    \n",
    "    This function creates a dense time grid at the specified interval spanning the input data's \n",
    "    time range, then performs forward-fill logic to populate missing timestamps with the last known values.\n",
    "    \n",
    "    Works in MICROSECONDS by default (aligns with Spark's native TimestampType precision).\n",
    "    \n",
    "    Args:\n",
    "        df (DataFrame): Source DataFrame with time-series data\n",
    "        cols (list of str): Column names to resample and forward-fill (value columns)\n",
    "        interval_us (int): Target resampling interval in MICROSECONDS.\n",
    "                          Examples:\n",
    "                          - 1_000_000 = 1 second\n",
    "                          - 5_000_000 = 5 seconds\n",
    "                          - 60_000_000 = 1 minute\n",
    "                          - 100_000 = 100 milliseconds\n",
    "                          - 1_000 = 1 millisecond\n",
    "                          - 1 = 1 microsecond\n",
    "        timestamp_col (str, optional): Name of the timestamp column. Defaults to \"ts\".\n",
    "        group_cols (list of str, optional): Grouping/ID columns to preserve (e.g., [\"exchange_id\", \"symbol\"]).\n",
    "                                           If provided, resampling is done separately for each group.\n",
    "                                           Defaults to None (no grouping).\n",
    "        return_as_timestamp (bool, optional): If True, converts the output timestamp column to Timestamp dtype.\n",
    "                                             If False, keeps as LONG in the same unit as input.\n",
    "                                             Defaults to False.\n",
    "        input_unit_us (float, optional): Conversion factor from input units to microseconds.\n",
    "                                        This is the number you MULTIPLY the input by to get microseconds.\n",
    "                                        Examples:\n",
    "                                        - 0.001 = input is in nanoseconds (divide by 1000)\n",
    "                                        - 1 = input is in microseconds (default, no conversion)\n",
    "                                        - 1_000 = input is in milliseconds (multiply by 1000)\n",
    "                                        - 1_000_000 = input is in seconds (multiply by 1,000,000)\n",
    "                                        Defaults to 1 (microseconds).\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame or None: Resampled DataFrame with specified interval resolution and forward-filled values.\n",
    "                          Returns None if input DataFrame is empty.\n",
    "    \n",
    "    Example:\n",
    "        >>> # Time unit constants (conversion factors to microseconds)\n",
    "        >>> NANO = 0.001        # nanoseconds: multiply by 0.001 (divide by 1000)\n",
    "        >>> US = 1              # microseconds: no conversion\n",
    "        >>> MS = 1_000          # milliseconds: multiply by 1000\n",
    "        >>> SECOND = 1_000_000  # seconds: multiply by 1,000,000\n",
    "        >>> \n",
    "        >>> # Interval constants (in microseconds)\n",
    "        >>> INTERVAL_1US = 1\n",
    "        >>> INTERVAL_1MS = 1_000\n",
    "        >>> INTERVAL_100MS = 100_000\n",
    "        >>> INTERVAL_1S = 1_000_000\n",
    "        >>> INTERVAL_1M = 60_000_000\n",
    "        \n",
    "        >>> # Example 1: Input in nanoseconds (high-frequency trading)\n",
    "        >>> resampled = resample_timeseries(\n",
    "        ...     nano_df,  # ts column: 1708185600123456789 (nanoseconds)\n",
    "        ...     [\"price\", \"volume\"], \n",
    "        ...     interval_us=INTERVAL_1MS,  # 1 millisecond intervals\n",
    "        ...     input_unit_us=NANO  # 0.001 - converts ns to us\n",
    "        ... )\n",
    "        >>> # Output: ts in nanoseconds (converted back)\n",
    "        \n",
    "        >>> # Example 2: Input in microseconds (default, best practice)\n",
    "        >>> resampled = resample_timeseries(\n",
    "        ...     trades_df,  # ts column: 1708185600123456 (microseconds)\n",
    "        ...     [\"price\", \"volume\"], \n",
    "        ...     interval_us=INTERVAL_1S  # 1 second intervals\n",
    "        ... )\n",
    "        >>> # Output: ts in microseconds\n",
    "        \n",
    "        >>> # Example 3: Input in milliseconds (very common)\n",
    "        >>> resampled = resample_timeseries(\n",
    "        ...     trades_df,  # ts column: 1708185600123 (milliseconds)\n",
    "        ...     [\"price\", \"volume\"], \n",
    "        ...     interval_us=INTERVAL_1S,\n",
    "        ...     input_unit_us=MS  # 1000\n",
    "        ... )\n",
    "        >>> # Output: ts in milliseconds (converted back)\n",
    "        \n",
    "        >>> # Example 4: Input in seconds (Unix timestamps)\n",
    "        >>> resampled = resample_timeseries(\n",
    "        ...     trades_df,  # ts column: 1708185600 (seconds)\n",
    "        ...     [\"price\", \"volume\"], \n",
    "        ...     interval_us=60 * INTERVAL_1S,  # 1 minute intervals\n",
    "        ...     input_unit_us=SECOND  # 1_000_000\n",
    "        ... )\n",
    "        >>> # Output: ts in seconds (converted back)\n",
    "        \n",
    "        >>> # Example 5: Return as Timestamp for human readability\n",
    "        >>> resampled = resample_timeseries(\n",
    "        ...     trades_df,\n",
    "        ...     [\"price\", \"volume\"],\n",
    "        ...     interval_us=INTERVAL_1S,\n",
    "        ...     input_unit_us=MS,\n",
    "        ...     return_as_timestamp=True\n",
    "        ... )\n",
    "        >>> # Output: ts as TimestampType (2024-02-17 12:00:00)\n",
    "        \n",
    "        >>> # Example 6: With grouping (multiple symbols)\n",
    "        >>> resampled = resample_timeseries(\n",
    "        ...     trades_df,\n",
    "        ...     [\"price\", \"volume\"],\n",
    "        ...     interval_us=INTERVAL_100MS,\n",
    "        ...     input_unit_us=MS,\n",
    "        ...     group_cols=[\"symbol\", \"exchange\"]\n",
    "        ... )\n",
    "        >>> # Output: ts, symbol, exchange, price, volume\n",
    "    \n",
    "    Notes:\n",
    "        - Uses LOCF (Last Observation Carried Forward) to fill gaps\n",
    "        - Works in MICROSECONDS internally (matches Spark's TimestampType precision)\n",
    "        - When group_cols is specified, each group gets its own complete time grid\n",
    "        - Output timestamps are in the same unit as input (unless return_as_timestamp=True)\n",
    "        - For nanosecond data: use input_unit_us=0.001 (sub-microsecond precision is lost)\n",
    "        - Conversion formula: microseconds = input_timestamp * input_unit_us\n",
    "    \"\"\"\n",
    "    if group_cols is None:\n",
    "        group_cols = []\n",
    "    \n",
    "    # Convert input timestamps to microseconds if needed\n",
    "    if input_unit_us != 1:\n",
    "        df = df.withColumn(timestamp_col, (F.col(timestamp_col) * input_unit_us).cast(\"long\"))\n",
    "    \n",
    "    # Get time bounds (overall or per group)\n",
    "    if group_cols:\n",
    "        # Get min/max per group\n",
    "        bounds = df.groupBy(*group_cols).agg(\n",
    "            F.min(timestamp_col).alias(\"min_ts\"),\n",
    "            F.max(timestamp_col).alias(\"max_ts\")\n",
    "        )\n",
    "        \n",
    "        if bounds.count() == 0:  # Empty DataFrame\n",
    "            return None\n",
    "        \n",
    "        # Create bucket boundaries per group\n",
    "        bounds = bounds.withColumn(\n",
    "            \"min_bucket\", (F.col(\"min_ts\") / interval_us).cast(\"long\") * interval_us\n",
    "        ).withColumn(\n",
    "            \"max_bucket\", (F.col(\"max_ts\") / interval_us).cast(\"long\") * interval_us\n",
    "        )\n",
    "        \n",
    "        # Generate grid per group using explode and sequence\n",
    "        grid = bounds.select(\n",
    "            *group_cols,\n",
    "            F.explode(\n",
    "                F.sequence(\n",
    "                    F.col(\"min_bucket\"),\n",
    "                    F.col(\"max_bucket\"),\n",
    "                    F.lit(interval_us)\n",
    "                )\n",
    "            ).alias(timestamp_col)\n",
    "        )\n",
    "    else:\n",
    "        # Get overall time bounds\n",
    "        mins = df.agg(\n",
    "            F.min(timestamp_col).alias(\"min_ts\"), \n",
    "            F.max(timestamp_col).alias(\"max_ts\")\n",
    "        ).first()\n",
    "        \n",
    "        if mins.min_ts is None:  # Empty DataFrame\n",
    "            return None\n",
    "        \n",
    "        # Calculate bucket boundaries (truncate to interval)\n",
    "        min_bucket = (mins.min_ts // interval_us) * interval_us\n",
    "        max_bucket = (mins.max_ts // interval_us) * interval_us\n",
    "        \n",
    "        # Generate complete time grid (sequence of buckets)\n",
    "        grid = spark.range(\n",
    "            min_bucket, \n",
    "            max_bucket + interval_us,  # +interval_us to include the last bucket\n",
    "            interval_us\n",
    "        ).select(F.col(\"id\").alias(timestamp_col))\n",
    "    \n",
    "    # Downsample source to target interval (keep last value per bucket and group)\n",
    "    # Create bucket column by dividing timestamp by interval\n",
    "    group_keys = group_cols + [\"bucket\"]\n",
    "    downsampled = df.withColumn(\"bucket\", (F.col(timestamp_col) / interval_us).cast(\"long\") * interval_us) \\\n",
    "        .groupBy(*group_keys) \\\n",
    "        .agg(*[F.last(c, ignorenulls=True).alias(c) for c in cols]) \\\n",
    "        .select(*group_cols, F.col(\"bucket\").alias(timestamp_col), *[F.col(c) for c in cols])\n",
    "    \n",
    "    # Join to grid\n",
    "    join_keys = group_cols + [timestamp_col] if group_cols else [timestamp_col]\n",
    "    joined = grid.join(downsampled, join_keys, \"left\")\n",
    "    \n",
    "    # Forward-fill missing values using window function\n",
    "    # Window is partitioned by group_cols (if any) and ordered by timestamp\n",
    "    if group_cols:\n",
    "        w = W.partitionBy(*group_cols).orderBy(timestamp_col).rowsBetween(W.unboundedPreceding, 0)\n",
    "    else:\n",
    "        w = W.orderBy(timestamp_col).rowsBetween(W.unboundedPreceding, 0)\n",
    "    \n",
    "    for c in cols:\n",
    "        joined = joined.withColumn(c, F.last(c, ignorenulls=True).over(w))\n",
    "    \n",
    "    # Convert output timestamps\n",
    "    if return_as_timestamp:\n",
    "        # Convert microseconds to Timestamp (perfect alignment!)\n",
    "        joined = joined.withColumn(\n",
    "            timestamp_col, \n",
    "            (F.col(timestamp_col) / 1_000_000).cast(\"timestamp\")\n",
    "        )\n",
    "    elif input_unit_us != 1:\n",
    "        # Convert back to original input unit\n",
    "        joined = joined.withColumn(\n",
    "            timestamp_col,\n",
    "            (F.col(timestamp_col) / input_unit_us).cast(\"long\")\n",
    "        )\n",
    "    \n",
    "    return joined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3eff8c58-3622-49b1-bec4-5c143d5bc89f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T20:21:45.211426Z",
     "iopub.status.busy": "2025-11-19T20:21:45.211207Z",
     "iopub.status.idle": "2025-11-19T20:21:48.481939Z",
     "shell.execute_reply": "2025-11-19T20:21:48.481311Z",
     "shell.execute_reply.started": "2025-11-19T20:21:45.211397Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10a9c685d4ee4280b25e8da8da94a0a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trades = spark.table(\"crypto_data.ethereum_trades\") \\\n",
    "    .where(F.col(\"dt\").between(F.lit(start_dt), F.lit(end_dt)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d545f656-9ba4-40a9-bc90-5b2f5d99f9e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T21:01:43.748747Z",
     "iopub.status.busy": "2025-11-19T21:01:43.748417Z",
     "iopub.status.idle": "2025-11-19T21:01:51.042750Z",
     "shell.execute_reply": "2025-11-19T21:01:51.042076Z",
     "shell.execute_reply.started": "2025-11-19T21:01:43.748705Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9a53cadeb474f5c9c8e95277b02931a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+------+-------------------+----+-------------------+----------+\n",
      "|        id|  price|amount|          timestamp|side|  receipt_timestamp|        dt|\n",
      "+----------+-------+------+-------------------+----+-------------------+----------+\n",
      "|1015897581| 2881.2|0.0101|1708300799997999872| buy|1708300800000819712|2024-02-19|\n",
      "|1015897582| 2881.2| 0.018|1708300799999000064| buy|1708300800008328704|2024-02-19|\n",
      "|1015897583| 2881.2|0.0156|1708300800000000000| buy|1708300800008368896|2024-02-19|\n",
      "|1015897584|2881.19|0.0199|1708300800000999936|sell|1708300800008384512|2024-02-19|\n",
      "|1015897585|2881.19|0.0142|1708300800000999936|sell|1708300800039269888|2024-02-19|\n",
      "|1015897586| 2881.2|0.0278|1708300800004000000| buy|1708300800039296000|2024-02-19|\n",
      "|1015897587|2881.19|0.0127|1708300800004000000|sell|1708300800292273152|2024-02-19|\n",
      "|1015897588| 2881.2| 0.009|1708300800005000192| buy|1708300800292348672|2024-02-19|\n",
      "|1015897589|2881.19|0.0352|1708300800005000192|sell|1708300800292365824|2024-02-19|\n",
      "|1015897590| 2881.2|0.0105|1708300800006000128| buy|1708300800292380160|2024-02-19|\n",
      "|1015897591| 2881.2|0.0251|1708300800006000128| buy|1708300800292393728|2024-02-19|\n",
      "|1015897592|2881.19|0.0295|1708300800006000128|sell|1708300800292406528|2024-02-19|\n",
      "|1015897593| 2881.2|0.0338|1708300800007000064| buy|1708300800292419584|2024-02-19|\n",
      "|1015897594|2881.19|0.0091|1708300800007000064|sell|1708300800292432128|2024-02-19|\n",
      "|1015897595| 2881.2|0.0173|1708300800007000064| buy|1708300800292444928|2024-02-19|\n",
      "|1015897596| 2881.2|0.0062|1708300800007000064| buy|1708300800292457472|2024-02-19|\n",
      "|1015897597|2881.19|0.0103|1708300800007000064|sell|1708300800292470528|2024-02-19|\n",
      "|1015897598| 2881.2|0.0142|1708300800008000000| buy|1708300800292483584|2024-02-19|\n",
      "|1015897599|2881.19|0.0093|1708300800008000000|sell|1708300800292496128|2024-02-19|\n",
      "|1015897600| 2881.2|0.0042|1708300800008000000| buy|1708300800292548608|2024-02-19|\n",
      "+----------+-------+------+-------------------+----+-------------------+----------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "trades.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7d3b875f-dc2d-477c-baa0-7613198045df",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T21:04:07.892569Z",
     "iopub.status.busy": "2025-11-19T21:04:07.892342Z",
     "iopub.status.idle": "2025-11-19T21:04:08.646191Z",
     "shell.execute_reply": "2025-11-19T21:04:08.645542Z",
     "shell.execute_reply.started": "2025-11-19T21:04:07.892544Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b75c058b91c41e2a09d955ce7b109f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6528794"
     ]
    }
   ],
   "source": [
    "trades.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2d4f23c4-7d2f-41fc-98a6-594a9681ea90",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T21:02:32.143382Z",
     "iopub.status.busy": "2025-11-19T21:02:32.143137Z",
     "iopub.status.idle": "2025-11-19T21:02:49.466746Z",
     "shell.execute_reply": "2025-11-19T21:02:49.466073Z",
     "shell.execute_reply.started": "2025-11-19T21:02:32.143351Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66bbdcbe5b704625821392cc85001bbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Usage examples:\n",
    "trades_1s = resample_timeseries(df=trades, cols=[\"price\", \"amount\", \"side\"], interval_us=1_000_000, timestamp_col=\"timestamp\", return_as_timestamp=True, input_unit_us=1/1_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8db1b2d0-474a-4c6e-89fc-d2e509b084a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T21:02:49.468456Z",
     "iopub.status.busy": "2025-11-19T21:02:49.467993Z",
     "iopub.status.idle": "2025-11-19T21:02:50.228606Z",
     "shell.execute_reply": "2025-11-19T21:02:50.227688Z",
     "shell.execute_reply.started": "2025-11-19T21:02:49.468416Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d78e341d3724ddeabdb7389e85dedc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "691200"
     ]
    }
   ],
   "source": [
    "trades_1s.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "233a8b81-665f-4085-a92b-87917b13180c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T21:04:20.488010Z",
     "iopub.status.busy": "2025-11-19T21:04:20.487774Z",
     "iopub.status.idle": "2025-11-19T21:04:22.756282Z",
     "shell.execute_reply": "2025-11-19T21:04:22.755581Z",
     "shell.execute_reply.started": "2025-11-19T21:04:20.487983Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57e8e2be95b54eec838688e25133be43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "An error occurred while calling o391.showString.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 6 in stage 21.0 failed 4 times, most recent failure: Lost task 6.3 in stage 21.0 (TID 105) (ip-172-31-20-64.eu-west-2.compute.internal executor 11): org.apache.spark.SparkException: Parquet column cannot be converted in file s3://ethereum-kbarczak/cryptolake/raw/trades/dt=2024-02-16/converted_fbb71ea129d14e4d96822abbe9739644.snappy.parquet. Column: [price], Expected: double, Found: FLOAT.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:856)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:442)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.$anonfun$hasNext$1(FileScanRDD.scala:272)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.util.FileAccessContext$.withContext(FileAccessContext.scala:69)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:272)\n",
      "\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:765)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.columnartorow_nextBatch_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.sort_addToSorter_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.sort_doSort_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:35)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.hasNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.aggregate.SortAggregateExec.$anonfun$doExecute$1(SortAggregateExec.scala:62)\n",
      "\tat org.apache.spark.sql.execution.aggregate.SortAggregateExec.$anonfun$doExecute$1$adapted(SortAggregateExec.scala:59)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:896)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:896)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:61)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:378)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:333)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:61)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:378)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:333)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.doWrite(ShuffleWriteProcessor.scala:45)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:69)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:174)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:152)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:633)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:636)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException: column: [price], physicalType: FLOAT, logicalType: double\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1136)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:199)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:175)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:355)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:246)\n",
      "\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.$anonfun$hasNext$1(FileScanRDD.scala:272)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.util.FileAccessContext$.withContext(FileAccessContext.scala:69)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:272)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:438)\n",
      "\t... 36 more\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:3083)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3019)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:3018)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3018)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1324)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1324)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1324)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3301)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3235)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3224)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.checkNoFailures(AdaptiveExecutor.scala:175)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.doRun(AdaptiveExecutor.scala:97)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.tryRunningAndGetFuture(AdaptiveExecutor.scala:75)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.execute(AdaptiveExecutor.scala:59)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$1(AdaptiveSparkPlanExec.scala:290)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:901)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.getFinalPhysicalPlan(AdaptiveSparkPlanExec.scala:289)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:596)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:558)\n",
      "\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4402)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3362)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4392)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:732)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4390)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:108)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:385)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.executeQuery$1(SQLExecution.scala:158)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$10(SQLExecution.scala:221)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:108)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:385)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$9(SQLExecution.scala:221)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:406)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:220)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:901)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:74)\n",
      "\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4390)\n",
      "\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3362)\n",
      "\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3585)\n",
      "\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:283)\n",
      "\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:318)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.SparkException: Parquet column cannot be converted in file s3://ethereum-kbarczak/cryptolake/raw/trades/dt=2024-02-16/converted_fbb71ea129d14e4d96822abbe9739644.snappy.parquet. Column: [price], Expected: double, Found: FLOAT.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:856)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:442)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.$anonfun$hasNext$1(FileScanRDD.scala:272)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.util.FileAccessContext$.withContext(FileAccessContext.scala:69)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:272)\n",
      "\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:765)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.columnartorow_nextBatch_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.sort_addToSorter_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.sort_doSort_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:35)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.hasNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.aggregate.SortAggregateExec.$anonfun$doExecute$1(SortAggregateExec.scala:62)\n",
      "\tat org.apache.spark.sql.execution.aggregate.SortAggregateExec.$anonfun$doExecute$1$adapted(SortAggregateExec.scala:59)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:896)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:896)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:61)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:378)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:333)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:61)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:378)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:333)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.doWrite(ShuffleWriteProcessor.scala:45)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:69)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:174)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:152)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:633)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:636)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\t... 1 more\n",
      "Caused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException: column: [price], physicalType: FLOAT, logicalType: double\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1136)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:199)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:175)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:355)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:246)\n",
      "\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.$anonfun$hasNext$1(FileScanRDD.scala:272)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.util.FileAccessContext$.withContext(FileAccessContext.scala:69)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:272)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:438)\n",
      "\t... 36 more\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/mnt/yarn/usercache/livy/appcache/application_1763581611380_0002/container_1763581611380_0002_01_000001/pyspark.zip/pyspark/sql/dataframe.py\", line 948, in show\n",
      "    print(self._show_string(n, truncate, vertical))\n",
      "  File \"/mnt/yarn/usercache/livy/appcache/application_1763581611380_0002/container_1763581611380_0002_01_000001/pyspark.zip/pyspark/sql/dataframe.py\", line 966, in _show_string\n",
      "    return self._jdf.showString(n, 20, vertical)\n",
      "  File \"/mnt/yarn/usercache/livy/appcache/application_1763581611380_0002/container_1763581611380_0002_01_000001/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/mnt/yarn/usercache/livy/appcache/application_1763581611380_0002/container_1763581611380_0002_01_000001/pyspark.zip/pyspark/errors/exceptions/captured.py\", line 179, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/mnt/yarn/usercache/livy/appcache/application_1763581611380_0002/container_1763581611380_0002_01_000001/py4j-0.10.9.7-src.zip/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o391.showString.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 6 in stage 21.0 failed 4 times, most recent failure: Lost task 6.3 in stage 21.0 (TID 105) (ip-172-31-20-64.eu-west-2.compute.internal executor 11): org.apache.spark.SparkException: Parquet column cannot be converted in file s3://ethereum-kbarczak/cryptolake/raw/trades/dt=2024-02-16/converted_fbb71ea129d14e4d96822abbe9739644.snappy.parquet. Column: [price], Expected: double, Found: FLOAT.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:856)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:442)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.$anonfun$hasNext$1(FileScanRDD.scala:272)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.util.FileAccessContext$.withContext(FileAccessContext.scala:69)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:272)\n",
      "\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:765)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.columnartorow_nextBatch_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.sort_addToSorter_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.sort_doSort_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:35)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.hasNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.aggregate.SortAggregateExec.$anonfun$doExecute$1(SortAggregateExec.scala:62)\n",
      "\tat org.apache.spark.sql.execution.aggregate.SortAggregateExec.$anonfun$doExecute$1$adapted(SortAggregateExec.scala:59)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:896)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:896)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:61)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:378)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:333)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:61)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:378)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:333)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.doWrite(ShuffleWriteProcessor.scala:45)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:69)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:174)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:152)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:633)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:636)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException: column: [price], physicalType: FLOAT, logicalType: double\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1136)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:199)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:175)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:355)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:246)\n",
      "\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.$anonfun$hasNext$1(FileScanRDD.scala:272)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.util.FileAccessContext$.withContext(FileAccessContext.scala:69)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:272)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:438)\n",
      "\t... 36 more\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:3083)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3019)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:3018)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3018)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1324)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1324)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1324)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3301)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3235)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3224)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.checkNoFailures(AdaptiveExecutor.scala:175)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.doRun(AdaptiveExecutor.scala:97)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.tryRunningAndGetFuture(AdaptiveExecutor.scala:75)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.execute(AdaptiveExecutor.scala:59)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$1(AdaptiveSparkPlanExec.scala:290)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:901)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.getFinalPhysicalPlan(AdaptiveSparkPlanExec.scala:289)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:596)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:558)\n",
      "\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4402)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3362)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4392)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:732)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4390)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:108)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:385)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.executeQuery$1(SQLExecution.scala:158)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$10(SQLExecution.scala:221)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:108)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:385)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$9(SQLExecution.scala:221)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:406)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:220)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:901)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:74)\n",
      "\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4390)\n",
      "\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3362)\n",
      "\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3585)\n",
      "\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:283)\n",
      "\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:318)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.SparkException: Parquet column cannot be converted in file s3://ethereum-kbarczak/cryptolake/raw/trades/dt=2024-02-16/converted_fbb71ea129d14e4d96822abbe9739644.snappy.parquet. Column: [price], Expected: double, Found: FLOAT.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:856)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:442)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.$anonfun$hasNext$1(FileScanRDD.scala:272)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.util.FileAccessContext$.withContext(FileAccessContext.scala:69)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:272)\n",
      "\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:765)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.columnartorow_nextBatch_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.sort_addToSorter_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.sort_doSort_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:35)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.hasNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.aggregate.SortAggregateExec.$anonfun$doExecute$1(SortAggregateExec.scala:62)\n",
      "\tat org.apache.spark.sql.execution.aggregate.SortAggregateExec.$anonfun$doExecute$1$adapted(SortAggregateExec.scala:59)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:896)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:896)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:61)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:378)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:333)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:61)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:378)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:333)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.doWrite(ShuffleWriteProcessor.scala:45)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:69)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:174)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:152)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:633)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:636)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\t... 1 more\n",
      "Caused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException: column: [price], physicalType: FLOAT, logicalType: double\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1136)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:199)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:175)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:355)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:246)\n",
      "\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.$anonfun$hasNext$1(FileScanRDD.scala:272)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.util.FileAccessContext$.withContext(FileAccessContext.scala:69)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:272)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:438)\n",
      "\t... 36 more\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trades_1s.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f30287e-9d0d-4185-b9c9-6a9ed678c168",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
